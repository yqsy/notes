---
title: 深入浅出dpdk
date: 2017-12-3 15:46:41
categories: [读书笔记]
---

<!-- TOC -->

- [1. p7-以太网接口技术的飞速发展](#1-p7-以太网接口技术的飞速发展)
- [2. p9-DPDK的历史](#2-p9-dpdk的历史)
- [3. p27-传统网络设备驱动包处理](#3-p27-传统网络设备驱动包处理)
- [4. p28-传统处理方式速度慢的原因](#4-p28-传统处理方式速度慢的原因)
- [5. p31-DPDK最佳实践](#5-p31-dpdk最佳实践)
- [6. p35-解读数据包处理能力](#6-p35-解读数据包处理能力)
- [7. p36-线速](#7-p36-线速)
- [8. p38-释放IA平台包处理的吞吐能力](#8-p38-释放ia平台包处理的吞吐能力)
- [9. p39-性能瓶颈不再是cpu](#9-p39-性能瓶颈不再是cpu)
- [10. p42-DPDK的方法论](#10-p42-dpdk的方法论)

<!-- /TOC -->

<a id="markdown-1-p7-以太网接口技术的飞速发展" name="1-p7-以太网接口技术的飞速发展"></a>
# 1. p7-以太网接口技术的飞速发展
以太网接口技术也经历了飞速发展。从早期主流的**10Mbit/s**与**100Mbit/s**，发展到千兆网（**1Gbit/s**）。到如今，万兆（**10Gbit/s**）网卡技术成为数据中心服务器的主流接口技术，近年来，Intel等公司还推出了**40Gbit/s**、**100Gbit/s**的超高速网络接口技术。**而CPU的运行频率基本停留在10年前的水平，为了迎接超高速网络技术的挑战，软件也需要大幅度创新。**

<a id="markdown-2-p9-dpdk的历史" name="2-p9-dpdk的历史"></a>
# 2. p9-DPDK的历史
早期CPU运行速度远高于外设访问，所以中断处理方式十分有效，但随着芯片技术与高速网络接口技术的一日千里式发展，报文吞吐需要高达**10Gbit/s**的端口处理能力，市面上已经出现大量的**25Gbit/s、40Gbit/s甚至100Gbit/s**高速端口，主流处理器的主频仍停留在3GHz以下。
高端游戏玩家可以将CPU超频到5GHz，但网络和通信节点的设计基于能效比经济性的考量，网络设备需要日以继夜地运行，运行成本（包含耗电量）在总成本中需要重点考量，**系统选型时大多选取2.5GHz以下的芯片**，保证合适的性价比。**I/O超越CPU的运行速率，是横在行业面前的技术挑战。用轮询来处理高速端口开始成为必然，这构成了DPDK运行的基础。**

  * 10Gbit/s 万兆网络,也就是 1.25GBytes/s
  * 100Gbit/s 十万兆网络,也就是 12.5GBytes/s

Intel起初只是将DPDK以源代码方式分享给少量客户，作为评估IA平台和硬件性能的软件服务模块，随着时间推移与行业的大幅度接受，2013年Intel将DPDK这一软件以BSD开源方式分享在Intel的网站上，供开发者免费下载。2013年4月，6wind联合其他开发者成立www.dpdk.org(http://www.dpdk.org)的开源社区，DPDK开始走上开源的大道。

<a id="markdown-3-p27-传统网络设备驱动包处理" name="3-p27-传统网络设备驱动包处理"></a>
# 3. p27-传统网络设备驱动包处理
  - 数据包到达网卡设备。
  - 网卡设备依据配置进行DMA操作。
  - 网卡发送中断，唤醒处理器。
  - 驱动软件填充读写缓冲区数据结构。
  - 数据报文达到内核协议栈，进行高层处理。
  - 如果最终应用在用户态，数据从内核搬移到用户态。
  - 如果最终应用在内核态，在内核继续进行。

<a id="markdown-4-p28-传统处理方式速度慢的原因" name="4-p28-传统处理方式速度慢的原因"></a>
# 4. p28-传统处理方式速度慢的原因
  * 随着网络接口带宽从千兆向万兆迈进，原先每个报文就会触发一个中断，中断带来的开销变得突出，**大量数据到来会触发频繁的中断开销，导致系统无法承受**，因此有人在Linux内核中引入了**NAPI机制**，其策略是**系统被中断唤醒后，尽量使用轮询的方式一次处理多个数据包**，直到网络再次空闲重新转入中断等待。NAPI策略用于高吞吐的场景，效率提升明显。

  * 即使在不需要协议处理的场景下，大多数场景下也需要把包从内核的缓冲区复制到用户缓冲区，**系统调用以及数据包复制的开销**，会直接影响用户态应用从设备直接获得包的能力 (有个著名的高性能网络I/O框架Netmap，它就是采用共享数据包池的方式，减少内核到用户空间的包复制。)

  * 以Netmap为例，即便其减少了内核到用户空间的内存复制，但**内核驱动的收发包处理和用户态线程依旧由操作系统调度执行**，除去任务切换本身的开销，**由切换导致的后续cache替换（不同任务内存热点不同），对性能也会产生负面的影响**。
<a id="markdown-5-p31-dpdk最佳实践" name="5-p31-dpdk最佳实践"></a>
# 5. p31-DPDK最佳实践

  * 轮询，这一点很直接，**可避免中断上下文切换的开销**。之前提到Linux也采用该方法改进对大吞吐数据的处理，效果很好

  * 用户态驱动，在这种工作方式下，**既规避了不必要的内存拷贝又避免了系统调用**。一个间接的影响在于，用户态驱动不受限于内核现有的数据格式和行为定义。对mbuf头格式的重定义、对网卡DMA操作的重新优化可以获得更好的性能。而用户态驱动也便于快速地迭代优化，甚至对不同场景进行不同的优化组合

  * 亲和性与独占，DPDK工作在用户态，线程的调度仍然依赖内核。**利用线程的CPU亲和绑定的方式，特定任务可以被指定只在某个核上工作**。好处是**可避免线程在不同核间频繁切换**，**核间线程切换容易导致因cache miss和cache write back造成的大量性能损失**。如果更进一步地限定某些核不参与Linux系统调度，就可能使线程独占该核，保证更多cache hit的同时，也避免了同一个核内的多任务切换开销。

  * 降低访存开销，网络数据包处理是一种典型的I/O密集型（I/O bound）工作负载。无论是CPU指令还是DMA，对于内存子系统（Cache+DRAM）都会访问频繁。利用一些已知的高效方法来减少访存的开销能够有效地提升性能。**比如利用内存大页能有效降低TLB miss，比如利用内存多通道的交错访问能有效提高内存访问的有效带宽，再比如利用对于内存非对称性的感知可以避免额外的访存延迟。**而cache更是几乎所有优化的核心地带

  * 软件调优，调优本身并不能说是最佳实践。这里其实指代的是一系列调优实践，比如**结构的cache line对齐**，比如**数据在多核间访问避免跨cache line共享**，**比如适时地预取数据，再如多元数据批量操作**。

  * **利用IA新硬件技术**，IA的最新指令集以及其他新功能一直是DPDK致力挖掘数据包处理性能的源泉。拿Intel<sup>®</sup>DDIO技术来讲，这个cache子系统对DMA访存的硬件创新直接助推了性能跨越式的增长。有效利用SIMD（Single Instruction Multiple Data）并结合超标量技术（Superscalar）对数据层面或者对指令层面进行深度并行化，在性能的进一步提升上也行之有效。另外一些指令（比如cmpxchg），本身就是lockless数据结构的基石，而crc32指令对与4Byte Key的哈希计算也是改善明显

  * 充分挖掘网卡的潜能，**经过DPDK I/O加速的数据包通过PCIe网卡进入系统内存，PCIe外设到系统内存之间的带宽利用效率、数据传送方式（coalesce操作）等都是直接影响I/O性能的因素。**在现代网卡中，往往还支持一些分流（如RSS，FDIR等）和卸载（如Chksum，TSO等）功能。DPDK充分利用这些硬件加速特性，帮助应用更好地获得直接的性能提升。

<a id="markdown-6-p35-解读数据包处理能力" name="6-p35-解读数据包处理能力"></a>
# 6. p35-解读数据包处理能力
不管什么样的硬件平台，对于包处理都有最基本的性能诉求。一般常被提到的有**吞吐、延迟、丢包率、抖动**等。**对于转发，常会以包转发率（pps，每秒包转发率）而不是比特率（bit/s，每秒比特转发率）来衡量转发能力**，这跟包在网络中传输的方式有关。不同大小的包对存储转发的能力要求不尽相同。让我们先来温习一下有效带宽和包转发率概念。


<a id="markdown-7-p36-线速" name="7-p36-线速"></a>
# 7. p36-线速
线速（Wire Speed）是**线缆中流过的帧理论上支持的最大帧数**。
我们用以太网（Ethernet）为例，**一般所说的接口带宽，1Gbit/s、10Gbit/s、25Gbit/s、40Gbit/s、100Gbit/s，代表以太接口线路上所能承载的最高传输比特率**，其单位是bit/s（bit per second，位/秒）。实际上，**不可能每个比特都传输有效数据**。以太网每个帧之间会有帧间距（Inter-Packet Gap，IPG），默认帧间距大小为12字节。每个帧还有7个字节的前导（Preamble），和1个字节的帧首定界符（Start Frame Delimiter，SFD）。

<a id="markdown-8-p38-释放ia平台包处理的吞吐能力" name="8-p38-释放ia平台包处理的吞吐能力"></a>
# 8. p38-释放IA平台包处理的吞吐能力
DPDK的出现充分释放了IA平台对包处理的吞吐能力。我们知道，随着吞吐率的上升，中断触发的开销是不能忍受的，DPDK通过一系列软件优化方法**（大页利用，cache对齐，线程绑定，NUMA感知，内存通道交叉访问，无锁化数据结构，预取，SIMD指令利用等）**利用IA平台硬件特性，提供完整的底层开发支持库。使得单核三层转发可以轻松地突破小包30Mpps，随着CPU封装的核数越来越多，支持的PCIe通道数越来越多，整系统的三层转发吞吐在2路CPU的Xeon E5-2658v3上可以达到300Mpps。

<a id="markdown-9-p39-性能瓶颈不再是cpu" name="9-p39-性能瓶颈不再是cpu"></a>
# 9. p39-性能瓶颈不再是cpu
DPDK软件包内有一个最基本的三层转发实例（l3fwd），可用于测试双路服务器整系统的吞吐能力，实验表明可以达到220Gbit/s的数据报文吞吐能力。值得注意的是，除了通过硬件或者软件提升性能之外，**如今DPDK整系统报文吞吐能力上限已经不再受限于CPU的核数，当前瓶颈在于PCIe（IO总线）的LANE数**。换句话说，**系统性能的整体I/O天花板不再是CPU，而是系统所提供的所有PCIe LANE的带宽，能插入多少个高速以太网接口卡。**

<a id="markdown-10-p42-dpdk的方法论" name="10-p42-dpdk的方法论"></a>
# 10. p42-DPDK的方法论

  * 专用负载下的针对性软件优化,专用处理器通过硬件架构专用优化来达到高性能，DPDK则利用通用处理器，通过优化的专用化底层软件来达到期望的高性能。这要求DPDK尽可能利用一切平台（CPU，芯片组，PCIe以及网卡）特性，并针对网络负载的特点，做针对性的优化，以发掘通用平台在某一专用领域的最大能力。

  * 追求可水平扩展的性能,**利用多核并行计算技术，提高性能和水平扩展能力**。对于产生的并发干扰，**遵循临界区越薄越好、临界区碰撞越少越好的指导原则**。**数据尽可能本地化和无锁化，追求吞吐率随核数增加而线性增长。**

  * 向Cache索求极致的实现优化性能,相比于系统优化和算法优化，实现优化往往较少被提及。**实现优化对开发者的要求体现在需要对处理器体系结构有所了解**。DPDK可谓集大量的实现优化之大成，而这些方法多数围绕着Cache进行，可以说能**娴熟地驾驭好Cache，在追求极致性能的路上就已经成功了一半。**


  * 理论分析结合实践推导,性能的天花板在哪，调优是否还有空间，是否值得花更多的功夫继续深入，这些问题有时很难直接找到答案。**分析、推测、做原型、跑数据、再分析，通过这样的螺旋式上升，慢慢逼近最优解，往往是实践道路上的导航明灯**。条件允许下，有依据的理论量化计算，可以更可靠地明确优化目标。

