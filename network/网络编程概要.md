

<!-- TOC -->

- [1. 应用层程序员注意哪些](#1-应用层程序员注意哪些)
- [2. 使用TCP传输可以多快](#2-使用tcp传输可以多快)
- [3. 每个连接占用多少资源](#3-每个连接占用多少资源)
- [4. 最大多少连接](#4-最大多少连接)
- [5. 使用tcp要做的3个事情](#5-使用tcp要做的3个事情)
- [6. 正确关闭连接](#6-正确关闭连接)
- [7. 心跳包怎么做](#7-心跳包怎么做)
- [8. 流分包的手段](#8-流分包的手段)
- [9. 应用层缓冲区是必须的](#9-应用层缓冲区是必须的)
- [10. 最大连接数限制](#10-最大连接数限制)
- [11. 定时器的手段](#11-定时器的手段)
- [12. 进程间通信手段](#12-进程间通信手段)
- [13. 三次握手 四次挥手 TIME_WAIT FIN_WAIT2](#13-三次握手-四次挥手-time_wait-fin_wait2)
- [14. SO_REUSEPORT and SO_REUSEADDR](#14-so_reuseport-and-so_reuseaddr)
- [15. 非阻塞event loop的模型的不足](#15-非阻塞event-loop的模型的不足)
- [16. socket多线程串话问题](#16-socket多线程串话问题)
- [17. 多线程谨慎使用fork](#17-多线程谨慎使用fork)
- [18. 多线程不要使用signal](#18-多线程不要使用signal)
- [19. 常见设计方案](#19-常见设计方案)
- [20. LT ET的思考](#20-lt-et的思考)
- [21. 其他汇总](#21-其他汇总)

<!-- /TOC -->

<a id="markdown-1-应用层程序员注意哪些" name="1-应用层程序员注意哪些"></a>
# 1. 应用层程序员注意哪些

* Etnernet framse 以太网: 帧
* IP packet : 分组 (不是分片)
* TCP segment: 段
* Application message: 消息


<a id="markdown-2-使用tcp传输可以多快" name="2-使用tcp传输可以多快"></a>
# 2. 使用TCP传输可以多快

* https://en.wikipedia.org/wiki/Data_rate_units (单位参考)
* https://en.wikipedia.org/wiki/Ethernet_frame (enthernet)
* https://en.wikipedia.org/wiki/IPv4 (ipv4)
* https://en.wikipedia.org/wiki/Transmission_Control_Protocol (tcp)

包头大小:
* Ethernet frame(without option) - 38B
* ipv4 header (without option) - 20B
* tcp header (without option) - 20B
* tcp option - 12B

1Gb Ethernet 带宽是多少? 125(MB/s) = 125,000,000(B/s) (Raw bandwidth)

Ethernet payload: 46-1500(B) 加上header: 84-1538(B)

每秒能传多少包?
* 每次都传输最小的包: 125,000,000 / 84 = 1,488,000 (packet/s)
* 每次都传输最大的包: 125,000,000 / 1538 = 81,274 (packet/s)

1Gb Ethernet下tcp最大吞吐量:
* 81,274 * (1500 - 52) = 117,684,752 = 117MB/S 或 112MiB/S


<a id="markdown-3-每个连接占用多少资源" name="3-每个连接占用多少资源"></a>
# 3. 每个连接占用多少资源

* https://zhuanlan.zhihu.com/p/25241630


<a id="markdown-4-最大多少连接" name="4-最大多少连接"></a>
# 4. 最大多少连接

* https://www.zhihu.com/question/66553828
* https://blog.csdn.net/solstice/article/details/6579232

<a id="markdown-5-使用tcp要做的3个事情" name="5-使用tcp要做的3个事情"></a>
# 5. 使用tcp要做的3个事情

* SO_REUSEADDR,监听复用地址
* 禁用SIGPIPE,防止收到客户端的关闭事件使服务端关闭
* 禁用Nagle算法,防止请求应答的延迟

<a id="markdown-6-正确关闭连接" name="6-正确关闭连接"></a>
# 6. 正确关闭连接

作为proxy,a->proxy->b:  
* 传透a->proxy->b的半关闭
* 传透a<-proxy<-b的半关闭
* close fd

client,server:  
* 协议设计的好,两边明确收发完所有消息,随便关闭
* 客户端主动FIN,服务端检查有没有数据需要发送.如果发送完毕,shutdown write,close
* 服务端发完数据后主动shutdown write,read 返回0, close

<a id="markdown-7-心跳包怎么做" name="7-心跳包怎么做"></a>
# 7. 心跳包怎么做

* https://www.zhihu.com/question/35896874/answer/116301692

为什么无法用keeplive代替心跳包?
* 负载均衡设备或代理无法穿透SO_KEEPLIVE
* 进程死锁,或阻塞,操作系统也能正常收发 SO_KEEPLIVE,无法说明应用程序还能正常工作

实现心跳包的思路?
* 发送周期
* 检查周期
* timeout

避免误报timeout取为2个检查周期(网络消息延时波动,定时器波动)`如果最近的心跳消息的接收时间早于now - 2Tc,可判断心跳失效`

还有消息堆积而产生假心跳的问题,可以把判断规则改成`如果最近的心跳消息的发送时间早于now - 2Tc,可判断心跳失效`

`受闰秒影响,Tc不要小于1s`

* 要在工作线程中发送,`不要单独起一个心跳线程` (工作线程死锁,还继续发心跳)
* 与业务消息用同一个连接,不`要单独用心跳连接` (防火墙在tcp没有数据交互时判断连接已关闭)

<a id="markdown-8-流分包的手段" name="8-流分包的手段"></a>
# 8. 流分包的手段

* 固定长度,例如roundtrip
* 特殊的字符边界,例如\r\n
* 头部加长度字段
* 利用消息格式本身来分包
 
 参考:
 * https://github.com/yqsy/recipes/tree/master/codec
 
<a id="markdown-9-应用层缓冲区是必须的" name="9-应用层缓冲区是必须的"></a>
# 9. 应用层缓冲区是必须的

其实不论阻塞非阻塞,应用层缓冲区都是必须的.

例如读: 协议为\r\n末尾时,难道从内核缓冲区一次拷贝一个字节判断是否是\r\n吗?要多少次系统调用!肯定是准备一个足够大的缓冲区一次尽可能的读到应用层  
例如写: 准备一大块数据调用系统接口send,在send拷贝到内核缓冲区之前,这块数据库不能被修改,只能在末尾append,这也就是写缓冲区


<a id="markdown-10-最大连接数限制" name="10-最大连接数限制"></a>
# 10. 最大连接数限制

描述符满了怎么做?  
继续accept或者connect会怎么样?  
怎么限制并发连接数  

* https://github.com/yqsy/recipes/tree/master/maxconnection
* https://golang.org/src/net/http/server.go (accept满了的做法,参考Serve函数)


<a id="markdown-11-定时器的手段" name="11-定时器的手段"></a>
# 11. 定时器的手段

Linux的记时函数:

函数|数据结构|精度|特点
-|-|-|-
time|time_t|秒|精度t太低
ftime|struct timeeb|毫秒|已被废弃
`gettimeofday`|struct timeval|微秒|`不是系统调用,用户态实现`
clock_gettime|struct timespec|纳秒|系统调用开销大

定时函数:

函数|特点
-|-
sleep/alarm/usleep|有可能用了SIGALRM,多线程处理信号很复杂
nanosleep/clock_nanosleep|阻塞
gettitimer/settitimer|用信号来deliver超时,多线程会有麻烦
timer_create/timer_settime/timer_gettime/timer_delete|用信号来deliver超时,多线程会有麻烦
`timerfd_create/timerfd_gettime/timerfd_settime`|`把时间变成了文件描述符,方便融入select,poll框架`


传统的Reactor定时功能的实现:  
使用select/poll/epoll的timeout来实现定时功能,但精度只有`毫秒`


<a id="markdown-12-进程间通信手段" name="12-进程间通信手段"></a>
# 12. 进程间通信手段

资料:
* https://en.wikipedia.org/wiki/Inter-process_communication

进程间通信方法:
* 匿名管道 (pipe)
* 具名管道 (FIFO)
* POSXI消息队列
* 共享内存
* 信号
* `Socket`

同步原语:
* `互斥器 mutex`
* `条件变量 condition variable`
* 读写锁 reader-writer lock
* 文件锁 record locking
* 信号量 semaphore


进程间通信选择`socket`:  
* 跨主机,伸缩性
* 双向的
* 不需要进程的父子关系
* 操作系统会回收资源
* 一个崩溃,另一个能够感知
* 配合tcpdump和wireshark分析吞吐量和延迟
* netcat/ss -ant 查看状态
* 如果带连接重试,那么连接是可再生的
* 吞吐量可观 (Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz下2GB/s)

IPC方式的吞吐量我没测试过,不过是否值得为了那么一点性能而去包装IPC呢,引入复杂的事物呢?

同步原语选择`mutex`,`条件变量`:

* 如果不能满足读的次数远多于写,就不能发挥读写锁的性能
* 信号量,与条件变量重合,但容易用错



<a id="markdown-13-三次握手-四次挥手-time_wait-fin_wait2" name="13-三次握手-四次挥手-time_wait-fin_wait2"></a>
# 13. 三次握手 四次挥手 TIME_WAIT FIN_WAIT2
![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/TCP_CLOSE.svg/260px-TCP_CLOSE.svg.png)

三次握手:  
引用知乎的一个回答:https://www.zhihu.com/question/24853633/answer/115072630  
```
让双方都证实对方能发收。
1：A发，B收， B知道A能发
2：B发，A收， A知道B能发收
3：A发，B收， B知道A能收
```

四次挥手:  
引用TCP/IP详解 卷1  
```
由TCP的半关闭(half-close)造成的,既然一个TCP连接是全双工的,因此每个方向必须单独进行关闭.
```

TIME_WAIT:  
引用TCP/IP详解 卷1  
```
当TCP执行一个主动关闭,并发送最后的ACK,该连接必须在TIME_WAIT状态停留的时间为2倍的MSL,让TCP再次发送最后的ACK以防这个ACK丢失
```

FIN_WAIT2:  
引用TCP/IP详解 卷1  
```
在FIN_WAIT2状态下,我们已经发出了FIN,并且另一端也已经对他进行确认.只有当另一端的进程完成这个关闭,我们这段才会从FIN_WAIT_2状态进入TIME_WAIT状态.

许多伯克利实现会在连接空闲一段时间后,将TCP进入CLOSED状态
```


<a id="markdown-14-so_reuseport-and-so_reuseaddr" name="14-so_reuseport-and-so_reuseaddr"></a>
# 14. SO_REUSEPORT and SO_REUSEADDR

* https://zhuanlan.zhihu.com/p/25533528
* https://lwn.net/Articles/542629/
* https://stackoverflow.com/questions/14388706

我的实践:
* https://github.com/yqsy/recipes/tree/master/so_reuseaddr
* https://github.com/yqsy/recipes/tree/master/so_reuseport

<a id="markdown-15-非阻塞event-loop的模型的不足" name="15-非阻塞event-loop的模型的不足"></a>
# 15. 非阻塞event loop的模型的不足

* 应用层控制流速
* 回调地狱
* 不能产生阻塞
* 使用第三方库时需要包装(提供select/poll/epoll的接口)
* 如果第三方库只能阻塞调用,需要多开线程去包装

<a id="markdown-16-socket多线程串话问题" name="16-socket多线程串话问题"></a>
# 16. socket多线程串话问题

问题:
```
线程A正准备 read socket时,线程B close socket, 线程C open了描述符,fd与线程A持有的fd相同

这时候A会把不属于自己的数据给取走,C会缺少想取的数据
```


c++的解决方法  
```
描述符关闭只由shared_ptr来管理,在没有任何线程去使用时才close,保证不会串话
```

go的解决方法  


```go
把状态保存在uint64中,使用atomic操作uint64,实现`close状态`,`读方向锁`,`写方向锁`
这样的好处是:
1. 线程A 阻塞读/写 -> 线程B close -> 线程A 被唤醒. 应用层调度层面的唤醒,讲究
2. 一旦描述符被close,状态会设置成closed,不会被close第二次,也无法被读写. 有考虑到这种情况,线程安全,使用起来放心

// fdMutex.state is organized as follows:
// 1 bit - whether netFD is closed, if set all subsequent lock operations will fail.
// 1 bit - lock for read operations.
// 1 bit - lock for write operations.
// 20 bits - total number of references (read+write+misc).
// 20 bits - number of outstanding read waiters.
// 20 bits - number of outstanding write waiters.
const (
	mutexClosed  = 1 << 0
	mutexRLock   = 1 << 1
	mutexWLock   = 1 << 2
	mutexRef     = 1 << 3
	mutexRefMask = (1<<20 - 1) << 3
	mutexRWait   = 1 << 23
	mutexRMask   = (1<<20 - 1) << 23
	mutexWWait   = 1 << 43
	mutexWMask   = (1<<20 - 1) << 43
)
```

<a id="markdown-17-多线程谨慎使用fork" name="17-多线程谨慎使用fork"></a>
# 17. 多线程谨慎使用fork

1. 子进程几乎继承了父进程的几乎全部状态,但是少数例外:
* 父进程的内存锁, mlock,mlockall
* 父进程的文件锁,fcntl
* 父进程的某些定时器,setitimer,alarm,timer_create

资源没有被继承到子进程,而子进程却继承了释放的代码,会发生什么?

2. fork只克隆当前线程的thread of control

其他线程正在持有某个锁,fork之后其他线程挂掉,当前线程马上死锁

<a id="markdown-18-多线程不要使用signal" name="18-多线程不要使用signal"></a>
# 18. 多线程不要使用signal

资料:
* https://en.wikipedia.org/wiki/Signal_(IPC)

原因:
* signal handler会打断正在运行的thread of control
* 修改变量必须是atomic的,因为编译器可能会优化内存访问

实践:
* 不主动处理异常信号(SIGTERM 15,SIGINT C-c 2),忽略SIGPIPE
* 把异步信号转换为同步的描述符事件
  * 传统做法: signal handler往pipe写字节,程序同步读取pipe (libevent貌似有)
  * 现代做法: signalfd把信号直接转换为文件描述符事件


<a id="markdown-19-常见设计方案" name="19-常见设计方案"></a>
# 19. 常见设计方案


方式|特点|优/缺点
-|-|-
accept+read/write|一次能够只服务一个客户|没有并发
accept+fork|每个客户fork一个进程|开销大
accept+thread|每个客户创建一个线程|开销大
pre fork|预先创建进程|额外逻辑
pre thread|预先创建线程|额外逻辑
poll (reactor)|单线程reactor `redis?`|只能发挥单核性能
reactor + thread pool|主线程IO,工作线程计算|发挥多核性能/可在线程池执行阻塞操作/单连接的顺序无保证
reactors in threads|one loop per thread`(muduo,memcached)`|减少了thread pool的切换,提高延时,适应性强
reactors in processes|`nginx`|没有共享资源,减少写代码的痛苦
reactors + thread pool|基于reactor + thread pool增加io线程|网络带宽大时,单线程event loop处理,不过比较少把

---
其中pre fork, pre thread还要思考谁来accept,`惊群`的问题,解决方案如下:

fork
* 无保护调用accept,会`有惊群`的问题
* `文件锁`保护accept
* `线程锁`(共享内存)保护accept
* 父进程accept,`管道`传递给子进程

thread
* `线程锁`
* 主线程accept,`queue`传递给工作线程


---

以上多种服务端模型从`容易编写`的`角度`去思考的话,应该`accept + thread`最容易编写了.我个人更倾向于在`应用态线程`投入更多的研究,因为技术发展的一种趋势是减少人类的心智负担.

<a id="markdown-20-lt-et的思考" name="20-lt-et的思考"></a>
# 20. LT ET的思考

LT是数据出发,ET是状态触发

收 -> LT更好
发 -> ET更好

<a id="markdown-21-其他汇总" name="21-其他汇总"></a>
# 21. 其他汇总

* https://zhuanlan.zhihu.com/p/20144829 (为什么TCP是个烂协议)
* https://zhuanlan.zhihu.com/p/30032980 (tcp协议专栏)
* http://www.cnxct.com/coping-with-the-tcp-time_wait-state-on-busy-linux-servers-in-chinese-and-dont-enable-tcp_tw_recycle/ (不要在linux上启用net.ipv4.tcp_tw_recycle)
